\begin{thebibliography}{10}

\bibitem{agarwal2011distributed}
A.~Agarwal and J.~C. Duchi.
\newblock Distributed delayed stochastic optimization.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  873--881, 2011.

\bibitem{allen2016katyusha}
Z.~Allen-Zhu.
\newblock Katyusha: Accelerated variance reduction for faster sgd.
\newblock {\em ArXiv e-prints, abs/1603.05953}, 2016.

\bibitem{bottou2010large}
L.~Bottou.
\newblock Large-scale machine learning with stochastic gradient descent.
\newblock In {\em Proceedings of COMPSTAT'2010}, pages 177--186. Springer,
  2010.

\bibitem{boyd2011distributed}
S.~Boyd, N.~Parikh, E.~Chu, B.~Peleato, and J.~Eckstein.
\newblock Distributed optimization and statistical learning via the alternating
  direction method of multipliers.
\newblock {\em Foundations and Trends{\textregistered} in Machine Learning},
  3(1):1--122, 2011.

\bibitem{defazio2016simple}
A.~Defazio.
\newblock A simple practical accelerated method for finite sums.
\newblock In {\em Advances In Neural Information Processing Systems}, pages
  676--684, 2016.

\bibitem{defazio2014saga}
A.~Defazio, F.~Bach, and S.~Lacoste-Julien.
\newblock Saga: A fast incremental gradient method with support for
  non-strongly convex composite objectives.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1646--1654, 2014.

\bibitem{Defazio2014}
A.~Defazio, F.~Bach, and S.~Lacoste-Julien.
\newblock {SAGA: A Fast Incremental Gradient Method With Support for
  Non-Strongly Convex Composite Objectives}.
\newblock {\em Nips}, pages 1--12, 2014.

\bibitem{gabay1976dual}
D.~Gabay and B.~Mercier.
\newblock A dual algorithm for the solution of nonlinear variational problems
  via finite element approximation.
\newblock {\em Computers \& Mathematics with Applications}, 2(1):17--40, 1976.

\bibitem{johnson2013accelerating}
R.~Johnson and T.~Zhang.
\newblock Accelerating stochastic gradient descent using predictive variance
  reduction.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  315--323, 2013.

\bibitem{lan2015optimal}
G.~Lan and Y.~Zhou.
\newblock An optimal randomized incremental gradient method.
\newblock {\em arXiv preprint arXiv:1507.02000}, 2015.

\bibitem{li2014scaling}
M.~Li, D.~G. Andersen, J.~W. Park, A.~J. Smola, A.~Ahmed, V.~Josifovski,
  J.~Long, E.~J. Shekita, and B.-Y. Su.
\newblock Scaling distributed machine learning with the parameter server.
\newblock In {\em OSDI}, volume~14, pages 583--598, 2014.

\bibitem{lian2015asynchronous}
X.~Lian, Y.~Huang, Y.~Li, and J.~Liu.
\newblock Asynchronous parallel stochastic gradient for nonconvex optimization.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2737--2745, 2015.

\bibitem{lin2014accelerated}
Q.~Lin, Z.~Lu, and L.~Xiao.
\newblock An accelerated proximal coordinate gradient method.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  3059--3067, 2014.

\bibitem{nemirovski2009robust}
A.~Nemirovski, A.~Juditsky, G.~Lan, and A.~Shapiro.
\newblock Robust stochastic approximation approach to stochastic programming.
\newblock {\em SIAM Journal on optimization}, 19(4):1574--1609, 2009.

\bibitem{ouyang2013stochastic}
H.~Ouyang, N.~He, L.~Tran, and A.~G. Gray.
\newblock Stochastic alternating direction method of multipliers.
\newblock {\em ICML (1)}, 28:80--88, 2013.

\bibitem{recht2011hogwild}
B.~Recht, C.~Re, S.~Wright, and F.~Niu.
\newblock Hogwild: A lock-free approach to parallelizing stochastic gradient
  descent.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  693--701, 2011.

\bibitem{roux2012stochastic}
N.~L. Roux, M.~Schmidt, and F.~R. Bach.
\newblock A stochastic gradient method with an exponential convergence \_rate
  for finite training sets.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2663--2671, 2012.

\bibitem{shalev2013accelerated}
S.~Shalev-Shwartz and T.~Zhang.
\newblock Accelerated mini-batch stochastic dual coordinate ascent.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  378--385, 2013.

\bibitem{shalev2013stochastic}
S.~Shalev-Shwartz and T.~Zhang.
\newblock Stochastic dual coordinate ascent methods for regularized loss
  minimization.
\newblock {\em Journal of Machine Learning Research}, 14(Feb):567--599, 2013.

\bibitem{shalev2014accelerated}
S.~Shalev-Shwartz and T.~Zhang.
\newblock Accelerated proximal stochastic dual coordinate ascent for
  regularized loss minimization.
\newblock In {\em ICML}, pages 64--72, 2014.

\bibitem{suzuki2013dual}
T.~Suzuki et~al.
\newblock Dual averaging and proximal gradient descent for online alternating
  direction multiplier method.
\newblock In {\em ICML (1)}, pages 392--400, 2013.

\bibitem{xiao2014proximal}
L.~Xiao and T.~Zhang.
\newblock A proximal stochastic gradient method with progressive variance
  reduction.
\newblock {\em SIAM Journal on Optimization}, 24(4):2057--2075, 2014.

\bibitem{yang2013trading}
T.~Yang.
\newblock Trading computation for communication: Distributed stochastic dual
  coordinate ascent.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  629--637, 2013.

\bibitem{yang2013analysis}
T.~Yang, S.~Zhu, R.~Jin, and Y.~Lin.
\newblock Analysis of distributed stochastic dual coordinate ascent.
\newblock {\em arXiv preprint arXiv:1312.1031}, 2013.

\bibitem{zhang2014asynchronous}
R.~Zhang and J.~T. Kwok.
\newblock Asynchronous distributed admm for consensus optimization.
\newblock In {\em ICML}, pages 1701--1709, 2014.

\bibitem{zhang2015stochastic}
Y.~Zhang and X.~Lin.
\newblock Stochastic primal-dual coordinate method for regularized empirical
  risk minimization.
\newblock In {\em ICML}, pages 353--361, 2015.

\bibitem{zinkevich2010parallelized}
M.~Zinkevich, M.~Weimer, L.~Li, and A.~J. Smola.
\newblock Parallelized stochastic gradient descent.
\newblock In {\em Advances in neural information processing systems}, pages
  2595--2603, 2010.

\end{thebibliography}
