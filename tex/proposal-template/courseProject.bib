Automatically generated by Mendeley Desktop 1.17.9
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@article{Bottou2010,
abstract = {During the last decade, the data sizes have grown faster than the speed of processors. In this context, the capabilities of statistical machine learning methods is limited by the computing time rather than the sample size. A more precise analysis uncovers qualitatively different tradeoffs for the case of small-scale and large-scale learning problems. The large-scale case involves the computational complexity of the underlying optimization algorithm in non-trivial ways. Unlikely optimization algorithms such as stochastic gradient descent show amazing performance for large-scale problems. In particular, second order stochastic gradient and averaged stochastic gradient are asymptotically efficient after a single pass on the training set.},
author = {Bottou, Le{\'{o}}n},
doi = {10.1007/978-3-7908-2604-3_16},
file = {:Users/mingrliu/Documents/Mendeley Desktop/compstat-2010.pdf:pdf},
isbn = {0269-2155},
issn = {0269-2155},
journal = {Proceedings of COMPSTAT'2010},
keywords = {efficiency,online learning,stochastic gradient descent},
pages = {177--186},
pmid = {20876631},
title = {{Large-Scale Machine Learning with Stochastic Gradient Descent}},
year = {2010}
}
@article{Jaggi2014,
abstract = {Communication remains the most significant bottleneck in the performance of distributed optimization algorithms for large-scale machine learning. In this paper, we propose a communication-efficient framework, CoCoA, that uses local computation in a primal-dual setting to dramatically reduce the amount of necessary communication. We provide a strong convergence rate analysis for this class of algorithms, as well as experiments on real-world distributed datasets with implementations in Spark. In our experiments, we find that as compared to state-of-the-art mini-batch versions of SGD and SDCA algorithms, CoCoA converges to the same .001-accurate solution quality on average 25x as quickly.},
archivePrefix = {arXiv},
arxivId = {arXiv:1409.1458v2},
author = {Jaggi, Martin and Terhorst, Jonathan and Smith, Virginia and Krishnan, Sanjay and Berkeley, U C and Tak, Martin and Jordan, Michael I and Hofmann, Thomas},
eprint = {arXiv:1409.1458v2},
file = {:Users/mingrliu/Documents/Mendeley Desktop/5599-communication-efficient-distributed-dual-coordinate-ascent.pdf:pdf},
issn = {10495258},
journal = {Nips},
pages = {1--9},
title = {{Communication-Efficient Distributed Dual Coordinate Ascent}},
year = {2014}
}
@article{Shalev-shwartz,
archivePrefix = {arXiv},
arxivId = {arXiv:1305.2581v1},
author = {Shalev-shwartz, Shai},
eprint = {arXiv:1305.2581v1},
file = {:Users/mingrliu/Documents/Mendeley Desktop/4938-accelerated-mini-batch-stochastic-dual-coordinate-ascent.pdf:pdf},
pages = {1--8},
title = {{Accelerated Mini-Batch Stochastic Dual Coordinate Ascent}}
}
@article{Chu2007,
abstract = {We are at the beginning of the multicore era. Computers will have increasingly many cores (processors), but there is still no good programming framework for these architectures, and thus no simple and unified way for machine learning to take advantage of the potential speed up. In this paper, we develop a broadly ap- plicable parallel programming method, one that is easily applied to many different learning algorithms. Our work is in distinct contrast to the tradition in machine learning of designing (often ingenious) ways to speed up a single algorithm at a time. Specifically, we show that algorithms that fit the Statistical Query model [15] can be written in a certain “summation form,” which allows them to be easily par- allelized on multicore computers. We adapt Google's map-reduce [7] paradigm to demonstrate this parallel speed up technique on a variety of learning algorithms including locally weighted linear regression (LWLR), k-means, logistic regres- sion (LR), naive Bayes (NB), SVM, ICA, PCA, gaussian discriminant analysis (GDA), EM, and backpropagation (NN). Our experimental results show basically linear speedup with an increasing number of processors.},
archivePrefix = {arXiv},
arxivId = {arXiv:1210.1833v2},
author = {Chu, Cheng-Tao and Kim, Sang Kyun and Lin, Yi-An and Yu, YuanYuan and Bradski, Gary and Ng, Andrew Y. and Olukotun, Kunle},
doi = {10.1234/12345678},
eprint = {arXiv:1210.1833v2},
file = {:Users/mingrliu/Documents/Mendeley Desktop/3150-map-reduce-for-machine-learning-on-multicore.pdf:pdf},
isbn = {0262195682},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems 19},
pages = {281--288},
pmid = {14199369},
title = {{Map-Reduce for Machine Learning on Multicore}},
year = {2007}
}
@article{Shalev-Shwartz2016a,
abstract = {Stochastic Dual Coordinate Ascent is a popular method for solving regularized loss minimization for the case of convex losses. We describe variants of SDCA that do not require explicit regularization and do not rely on duality. We prove linear convergence rates even if individual loss functions are non-convex, as long as the expected loss is strongly convex.},
archivePrefix = {arXiv},
arxivId = {1602.01582},
author = {Shalev-Shwartz, Shai},
eprint = {1602.01582},
file = {:Users/mingrliu/Documents/Mendeley Desktop/shalev-shwartza16.pdf:pdf},
isbn = {9781510829008},
journal = {Proceedings of The 33rd International Conference on Machine Learning},
pages = {747--754},
title = {{SDCA without Duality, Regularization, and Individual Convexity}},
url = {http://arxiv.org/abs/1602.01582},
volume = {48},
year = {2016}
}
@article{Johnson2013,
abstract = {Stochastic gradient descent is popular for large scale optimization but has slow convergence asymptotically due to the inherent variance. To remedy this prob-lem, we introduce an explicit variance reduction method for stochastic gradient descent which we call stochastic variance reduced gradient (SVRG). For smooth and strongly convex functions, we prove that this method enjoys the same fast con-vergence rate as those of stochastic dual coordinate ascent (SDCA) and Stochastic Average Gradient (SAG). However, our analysis is significantly simpler and more intuitive. Moreover, unlike SDCA or SAG, our method does not require the stor-age of gradients, and thus is more easily applicable to complex problems such as some structured prediction problems and neural network learning.},
author = {Johnson, Rie and Zhang, Tong},
file = {:Users/mingrliu/Documents/Mendeley Desktop/4937-accelerating-stochastic-gradient-descent-using-predictive-variance-reduction.pdf:pdf},
issn = {10495258},
journal = {Nips},
keywords = {To-Read},
number = {3},
pages = {315--323},
title = {{Accelerating Stochastic Gradient Descent using Predictive Variance Reduction}},
url = {http://papers.nips.cc/paper/4937-accelerating-stochastic-gradient-descent-using-predictive-variance-reduction.pdf{\%}5Cnhttp://papers.nips.cc/paper/4937-accelerating-stochastic-gradient-descent-using-predictive-variance-reduction{\%}5CnAll Papers/J/Johnson and Zhan},
volume = {1},
year = {2013}
}
@article{Lan2000,
abstract = {In this paper, we consider a class of finite-sum convex optimization problems whose objective function is given by the summation of m (≥ 1) smooth components together with some other relatively simple terms. We first introduce a deterministic primal-dual gradient (PDG) method that can achieve the optimal black-box iteration complexity for solving these composite optimization problems using a primal-dual termination criterion. Our major contribution is to develop a randomized primal-dual gradient (RPDG) method, which needs to compute the gradient of only one randomly selected smooth component at each iteration, but can possibly achieve better complexity than PDG in terms of the total number of gradient evaluations. More specifically, we show that the total number of gradient evaluations performed by RPDG can beO(√m) times smaller, both in expectation and with high probability, than those performed by deterministic optimal first-order methods under favorable situations. We also show that the complexity of the RPDG method is not improvable by developing a new lower complexity bound for a general class of randomized methods for solving large-scale finite-sum convex optimization problems. Moreover, through the development of PDG and RPDG, we introduce a novel game-theoretic interpretation for these optimal methods for convex optimization.},
archivePrefix = {arXiv},
arxivId = {arXiv:1507.02000v2},
author = {Lan, Guanghui and Zhou, Yi},
eprint = {arXiv:1507.02000v2},
file = {:Users/mingrliu/Documents/Mendeley Desktop/1507.02000.pdf:pdf},
keywords = {Nesterov's method,complexity,convex programming,data analysis,incremental gradient,primal-dual gradient method},
pages = {1--33},
title = {{An optimal randomized incremental gradient method}},
year = {2000}
}
@article{Csiba2015,
abstract = {This paper introduces AdaSDCA: an adaptive variant of stochastic dual coordinate ascent (SDCA) for solving the regularized empirical risk minimization problems. Our modification consists in allowing the method adaptively change the probability distribution over the dual variables throughout the iterative process. AdaSDCA achieves provably better complexity bound than SDCA with the best fixed probability distribution, known as importance sampling. However, it is of a theoretical character as it is expensive to implement. We also propose AdaSDCA+: a practical variant which in our experiments outperforms existing non-adaptive methods.},
archivePrefix = {arXiv},
arxivId = {1502.08053},
author = {Csiba, Dominik and Qu, Zheng and Richtarik, Peter},
eprint = {1502.08053},
file = {:Users/mingrliu/Documents/Mendeley Desktop/csiba15.pdf:pdf},
isbn = {1502.08053},
journal = {Proceedings of The 32nd International Conference on Machine Learning},
number = {2},
pages = {674--683},
title = {{Stochastic Dual Coordinate Ascent with Adaptive Probabilities}},
url = {http://jmlr.org/proceedings/papers/v37/csiba15.html},
volume = {37},
year = {2015}
}
@article{Zinkevich2010,
abstract = {With the increase in available data parallel machine learning has become an increasingly pressing problem. In this paper we present the first parallel stochastic gradient descent algorithm including a detailed analysis and experimental evidence. Unlike prior work on parallel optimization algorithms our variant comes with parallel acceleration guarantees and it poses no overly tight latency constraints, which might only be available in the multicore setting. Our analysis introduces a novel proof technique --- contractive mappings to quantify the speed of convergence of parameter distributions to their asymptotic limits. As a side effect this answers the question of how quickly stochastic gradient descent algorithms reach the asymptotically normal regime.},
author = {Zinkevich, Martin a and Smola, Alex and Weimer, Markus},
doi = {10.1088/0741-3335/38/11/011},
file = {:Users/mingrliu/Documents/Mendeley Desktop/4006-parallelized-stochastic-gradient-descent.pdf:pdf},
issn = {07413335},
journal = {Proceedings of Advances in Neural Information Processing Systems (NIPS)},
pages = {1--9},
pmid = {12484348},
title = {{Parallelized stochastic gradient descent}},
url = {http://papers.nips.cc/paper/4006-parallelized-stochastic-gradient-descent},
volume = {23},
year = {2010}
}
@article{Suzuki2013,
abstract = {We develop new stochastic optimization methods that are applicable to a wide range of structured regularizations. Basically our methods are combinations of basic stochastic optimization techniques and Alternating Di- rectionMultiplierMethod (ADMM). ADMM is a general framework for optimizing a com- posite function, and has a wide range of ap- plications. We propose two types of online variants of ADMM, which correspond to on- line proximal gradient descent and regular- ized dual averaging respectively. The pro- posed algorithms are computationally effi- cient and easy to implement. Our methods yield O(1/√T) convergence of the expected risk. Moreover, the online proximal gradi- ent descent type method yields O(log(T)/T) convergence for a strongly convex loss. Nu- merical experiments show effectiveness of our methods in learning tasks with structured sparsity such as overlapped group lasso.},
author = {Suzuki, Taiji},
file = {:Users/mingrliu/Documents/Mendeley Desktop/suzuki13.pdf:pdf},
journal = {Proceedings of the 30th International Conference on Machine Learning (ICML)},
keywords = {Alternating Direction Multiplier Method,Dual Averaging,Online Learning,Stochastic Gradient Descent,Stochastic Optimization},
title = {{Dual Averaging and Proximal Gradient Descent for Online Alternating Direction Multiplier Method}},
volume = {28},
year = {2013}
}
@article{Pal2016,
abstract = {In prior works, stochastic dual coordinate ascent (SDCA) has been parallelized in a multi-core environment where the cores communicate through shared memory, or in a multi-processor distributed memory environment where the processors communicate through message passing. In this paper, we propose a hybrid SDCA framework for multi-core clusters, the most common high performance computing environment that consists of multiple nodes each having multiple cores and its own shared memory. We distribute data across nodes where each node solves a local problem in an asynchronous parallel fashion on its cores, and then the local updates are aggregated via an asynchronous across-node update scheme. The proposed double asynchronous method converges to a global solution for {\$}L{\$}-Lipschitz continuous loss functions, and at a linear convergence rate if a smooth convex loss function is used. Extensive empirical comparison has shown that our algorithm scales better than the best known shared-memory methods and runs faster than previous distributed-memory methods. Big datasets, such as one of 280 GB from the LIBSVM repository, cannot be accommodated on a single node and hence cannot be solved by a parallel algorithm. For such a dataset, our hybrid algorithm takes 30 seconds to achieve a duality gap of {\$}10{\^{}}{\{}-6{\}}{\$} on 16 nodes each using 8 cores, which is significantly faster than the best known distributed algorithms, such as CoCoA+, that take more than 300 seconds on 16 nodes.},
archivePrefix = {arXiv},
arxivId = {1610.07184},
author = {Pal, Soumitra and Xu, Tingyang and Yang, Tianbao and Rajasekaran, Sanguthevar and Bi, Jinbo},
eprint = {1610.07184},
file = {:Users/mingrliu/Documents/Mendeley Desktop/1610.07184.pdf:pdf},
keywords = {distributed computing,dual coordinate descent,optimization},
title = {{Hybrid-DCA: A Double Asynchronous Approach for Stochastic Dual Coordinate Ascent}},
url = {http://arxiv.org/abs/1610.07184},
year = {2016}
}
@article{Bradley2011,
abstract = {We propose Shotgun, a parallel coordinate descent algorithm for minimizing L1-regularized losses. Though coordinate descent seems inherently sequential, we prove convergence bounds for Shotgun which predict linear speedups, up to a problem-dependent limit. We present a comprehensive empirical study of Shotgun for Lasso and sparse logistic regression. Our theoretical predictions on the potential for parallelism closely match behavior on real data. Shotgun outperforms other published solvers on a range of large problems, proving to be one of the most scalable algorithms for L1.},
archivePrefix = {arXiv},
arxivId = {1105.5379},
author = {Bradley, Joseph K and Kyrola, Aapo and Bickson, Danny and Guestrin, Carlos},
eprint = {1105.5379},
file = {:Users/mingrliu/Documents/Mendeley Desktop/icml2011-bradley-kyrola-bickson-guestrin.pdf:pdf},
isbn = {9781450306195},
journal = {International Conference on Machine Learning},
number = {1998},
pages = {321--328},
title = {{Parallel Coordinate Descent for L1-Regularized Loss Minimization}},
year = {2011}
}
@article{Zhang2014,
abstract = {Distributed optimization algorithms are highly attractive for solving big data problems. In particular, many machine learning problems can be formulated as the global consensus optimization problem, which can then be solved in a distributed manner by the alternating direction method of multipliers (ADMM) algorithm. However, this suffers from the straggler problem as its updates have to be synchronized. In this paper, we propose an asynchronous ADMM algorithm by using two conditions to control the asynchrony: partial barrier and bounded delay. The proposed algorithm has a simple structure and good convergence guarantees (its convergence rate can be reduced to that of its synchronous counterpart). Experiments on different distributed ADMM applications show that asynchrony reduces the time on network waiting, and achieves faster convergence than its synchronous counterpart in terms of the wall clock time.},
author = {Zhang, Ruiliang and Kwok, James T.},
file = {:Users/mingrliu/Documents/Mendeley Desktop/zhange14.pdf:pdf},
isbn = {9781634393973},
journal = {31st International Conference on Machine Learning, ICML 2014},
number = {2},
pages = {3689--3697},
title = {{Asynchronous distributed ADMM for consensus optimization}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84919796967{\&}partnerID=tZOtx3y1},
volume = {5},
year = {2014}
}
@article{Allen-Zhu2016,
abstract = {We introduce {\$}\backslashmathtt{\{}Katyusha{\}}{\$}, the first direct stochastic gradient method that has an accelerated convergence rate. Given an objective that is an average of {\$}n{\$} convex and smooth functions, {\$}\backslashmathtt{\{}Katyusha{\}}{\$} converges to an {\$}\backslashvarepsilon{\$}-approximate minimizer using {\$}O((n + \backslashsqrt{\{}n \backslashkappa{\}})\backslashcdot \backslashlog\backslashfrac{\{}f(x{\_}0)-f(x{\^{}}*){\}}{\{}\backslashvarepsilon{\}}){\$} stochastic iterations, where {\$}\backslashkappa{\$} is the condition number. {\$}\backslashmathtt{\{}Katyusha{\}}{\$} is a direct primal method. In contrast, previous accelerated stochastic methods are either based on dual coordinate descent which are more restrictive, or based on outer-inner loops which make them "blind" to the underlying stochastic nature of the optimization process. {\$}\backslashmathtt{\{}Katyusha{\}}{\$} is the first algorithm that incorporates acceleration directly into the stochastic gradient updates. {\$}\backslashmathtt{\{}Katyusha{\}}{\$} supports proximal updates, non-Euclidean norm smoothness, non-uniform sampling, as well as mini-batch sampling. It also improves the best known convergence rates on many interesting classes of convex objectives, including smooth objectives (e.g., Lasso, Logistic Regression), strongly-convex objectives (e.g., SVM), and non-smooth objectives (e.g., L1SVM). The main ingredient behind our result is Katyusha momentum, a clever "negative momentum on top of momentum" that can be added on top of a variance-reduction based algorithm and speed it up. As a result, since variance reduction has been successfully applied to a fast growing list of practical problems, our paper suggests that in each of such cases, one had better hurry up and give Katyusha a hug.},
archivePrefix = {arXiv},
arxivId = {1603.05953},
author = {Allen-Zhu, Zeyuan},
eprint = {1603.05953},
file = {:Users/mingrliu/Documents/Mendeley Desktop/1603.05953.pdf:pdf},
journal = {arXiv},
keywords = {Acceleration Method,SGD},
number = {May},
pages = {1--18},
title = {{Katyusha: The First Direct Acceleration of Stochastic Gradient Methods}},
url = {http://arxiv.org/abs/1603.05953},
year = {2016}
}
@article{Defazio2014,
abstract = {In this work we introduce a new optimisation method called SAGA in the spirit of SAG, SDCA, MISO and SVRG, a set of recently proposed incremental gradient algorithms with fast linear convergence rates. SAGA improves on the theory behind SAG and SVRG, with better theoretical convergence rates, and has support for composite objectives where a proximal operator is used on the regulariser. Unlike SDCA, SAGA supports non-strongly convex problems directly, and is adaptive to any inherent strong convexity of the problem. We give experimental results showing the effectiveness of our method.},
archivePrefix = {arXiv},
arxivId = {arXiv:1407.0202v2},
author = {Defazio, Aaron and Bach, Francis and Lacoste-Julien, S},
eprint = {arXiv:1407.0202v2},
file = {:Users/mingrliu/Documents/Mendeley Desktop/5258-saga-a-fast-incremental-gradient-method-with-support-for-non-strongly-convex-composite-objectives.pdf:pdf},
issn = {10495258},
journal = {Nips},
pages = {1--12},
title = {{SAGA: A Fast Incremental Gradient Method With Support for Non-Strongly Convex Composite Objectives}},
url = {http://arxiv.org/abs/1407.0202},
year = {2014}
}
@article{Ouyang2013,
author = {Ouyang, Hua and He, Niao and Tran, Long and Gray, Alexander},
file = {:Users/mingrliu/Documents/Mendeley Desktop/ouyang13.pdf:pdf},
journal = {Proceedings of the 30th International Conference on Machine Learning},
pages = {80--88},
title = {{Stochastic alternating direction method of multipliers}},
url = {http://jmlr.csail.mit.edu/proceedings/papers/v28/ouyang13.pdf},
year = {2013}
}
@article{nemirovski2009robust,
	title={Robust stochastic approximation approach to stochastic programming},
	author={Nemirovski, Arkadi and Juditsky, Anatoli and Lan, Guanghui and Shapiro, Alexander},
	journal={SIAM Journal on optimization},
	volume={19},
	number={4},
	pages={1574--1609},
	year={2009},
	publisher={SIAM}
}
@article{Yang2014,
abstract = {In $\backslash$citep{\{}Yangnips13{\}}, the author presented distributed stochastic dual coordinate ascent (DisDCA) algorithms for solving large-scale regularized loss minimization. Extraordinary performances have been observed and reported for the well-motivated updates, as referred to the practical updates, compared to the naive updates. However, no serious analysis has been provided to understand the updates and therefore the convergence rates. In the paper, we bridge the gap by providing a theoretical analysis of the convergence rates of the practical DisDCA algorithm. Our analysis helped by empirical studies has shown that it could yield an exponential speed-up in the convergence by increasing the number of dual updates at each iteration. This result justifies the superior performances of the practical DisDCA as compared to the naive variant. As a byproduct, our analysis also reveals the convergence behavior of the one-communication DisDCA.},
archivePrefix = {arXiv},
arxivId = {1312.1031},
author = {Yang, Tianbao and Zhu, Shenghuo and Jin, Rong and Lin, Yuanqing},
eprint = {1312.1031},
file = {:Users/mingrliu/Documents/Mendeley Desktop/1312.1031.pdf:pdf},
journal = {Technical Report，Department of Computer Science, University of Iowa},
pages = {1--13},
title = {{Analysis of Distributed Stochastic Dual Coordinate Ascent}},
url = {http://arxiv.org/abs/1312.1031},
year = {2014}
}
@article{Yu2015,
abstract = {We proposed a doubly stochastic primal-dual coordinate optimization algorithm for regularized empirical risk minimization that can be formulated as a saddle-point problem using conjugate function. Different from existing coordinate methods, the proposed method randomly samples both primal and dual coordinates to update solutions, which is a desirable property when applied to data with both a high dimension and a large size. The convergence of our method is established not only in terms of the solution's distance to optimality but also in terms of the primal-dual objective gap. When applied to the data matrix factorized as a product of two smaller matrices, we show that the proposed method has a lower overall complexity than other coordinate methods, especially, when data size is large.},
archivePrefix = {arXiv},
arxivId = {1508.03390},
author = {Yu, Adams Wei and Lin, Qihang and Yang, Tianbao},
eprint = {1508.03390},
file = {:Users/mingrliu/Documents/Mendeley Desktop/zhanga15.pdf:pdf},
pages = {1--25},
title = {{Doubly Stochastic Primal-Dual Coordinate Method for Regularized Empirical Risk Minimization with Factorized Data}},
url = {http://arxiv.org/abs/1508.03390},
volume = {37},
year = {2015}
}
@article{Boyd2011,
author = {Boyd, Stephen P},
file = {:Users/mingrliu/Documents/Mendeley Desktop/finalResult{\_}updated.pdf:pdf},
isbn = {160198460X (pbk.)$\backslash$r9781601984609 (pbk.)},
keywords = {Algorithms.,Convex programming.,Machine learning.},
pages = {126 p.},
pmid = {17504609},
title = {{Distributed optimization and statistical learning via the alternating direction method of multipliers}},
year = {2011}
}
@article{Shalev-Shwartz2016,
abstract = {Stochastic Gradient Descent (SGD) has become popular for solving large scale supervised machine learning optimization problems such as SVM, due to their strong theoretical guarantees. While the closely related Dual Coordinate Ascent (DCA) method has been implemented in various software packages, it has so far lacked good convergence analysis. This paper presents a new analysis of Stochastic Dual Coordinate Ascent (SDCA) showing that this class of methods enjoy strong theo-retical guarantees that are comparable or better than SGD. This analysis justifies the effectiveness of SDCA for practical applications.},
archivePrefix = {arXiv},
arxivId = {1209.1873},
author = {Shalev-Shwartz, Shai and Zhang, Tong},
doi = {10.1007/s10107-014-0839-0},
eprint = {1209.1873},
file = {:Users/mingrliu/Documents/Mendeley Desktop/shalev-shwartz13a.pdf:pdf},
isbn = {1532-4435},
issn = {14364646},
journal = {Mathematical Programming},
keywords = {90C06,90C15,90C25},
number = {1-2},
pages = {105--145},
pmid = {2079951},
title = {{Accelerated proximal stochastic dual coordinate ascent for regularized loss minimization}},
volume = {155},
year = {2016}
}
@article{Agarwal2012,
abstract = {We analyze the convergence of gradient-based optimization algorithms that base their updates on delayed stochastic gradient information. The main application of our results is to the development of gradient-based distributed optimization algorithms where a master node performs parameter updates while worker nodes compute stochastic gradients based on local information in parallel, which may give rise to delays due to asynchrony. We take motivation from statistical problems where the size of the data is so large that it cannot fit on one computer; with the advent of huge datasets in biology, astronomy, and the internet, such problems are now common. Our main contribution is to show that for smooth stochastic problems, the delays are asymptotically negligible and we can achieve order-optimal convergence results. In application to distributed optimization, we develop procedures that overcome communication bottlenecks and synchronization requirements. We show {\$}n{\$}-node architectures whose optimization error in stochastic problems---in spite of asynchronous delays---scales asymptotically as {\$}\backslashorder(1 / \backslashsqrt{\{}nT{\}}){\$} after {\$}T{\$} iterations. This rate is known to be optimal for a distributed system with {\$}n{\$} nodes even in the absence of delays. We additionally complement our theoretical results with numerical experiments on a statistical machine learning task.},
archivePrefix = {arXiv},
arxivId = {1104.5525},
author = {Agarwal, Alekh and Duchi, John C.},
doi = {10.1109/CDC.2012.6426626},
eprint = {1104.5525},
file = {:Users/mingrliu/Documents/Mendeley Desktop/4247-distributed-delayed-stochastic-optimization.pdf:pdf},
isbn = {978-1-4673-2066-5},
issn = {01912216},
journal = {Proceedings of the IEEE Conference on Decision and Control},
pages = {5451--5452},
title = {{Distributed delayed stochastic optimization}},
year = {2012}
}
@article{Duchi2015,
abstract = {We show that asymptotically, completely asynchronous stochastic gradient procedures achieve optimal (even to constant factors) convergence rates for the solution of convex optimization prob-lems under nearly the same conditions required for asymptotic optimality of standard stochastic gradient procedures. Roughly, the noise inherent to the stochastic approximation scheme dom-inates any noise from asynchrony. We also give empirical evidence demonstrating the strong performance of asynchronous, parallel stochastic optimization schemes, demonstrating that the robustness inherent to stochastic approximation problems allows substantially faster parallel and asynchronous solution methods.},
archivePrefix = {arXiv},
arxivId = {arXiv:1508.00882v1},
author = {Duchi, John C and Chaturapruek, Sorathan and R{\'{e}}, Christopher},
eprint = {arXiv:1508.00882v1},
file = {:Users/mingrliu/Documents/Mendeley Desktop/1508.00882.pdf:pdf},
keywords = {()},
number = {i},
pages = {1--38},
title = {{Asynchronous stochastic convex optimization}},
year = {2015}
}
@article{Niu2011,
abstract = {Stochastic Gradient Descent (SGD) is a popular algorithm that can achieve state-of-the-art performance on a variety of machine learning tasks. Several researchers have recently proposed schemes to parallelize SGD, but all require performance-destroying memory locking and synchronization. This work aims to show using novel theoretical analysis, algorithms, and implementation that SGD can be implemented without any locking. We present an update scheme called HOGWILD! which allows processors access to shared memory with the possibility of overwriting each other's work. We show that when the associated optimization problem is sparse, meaning most gradient updates only modify small parts of the decision variable, then HOGWILD! achieves a nearly optimal rate of convergence. We demonstrate experimentally that HOGWILD! outperforms alternative schemes that use locking by an order of magnitude.},
archivePrefix = {arXiv},
arxivId = {1106.5730},
author = {Niu, Feng and Recht, Benjamin and Re, Christopher and Wright, Stephen J.},
eprint = {1106.5730},
file = {:Users/mingrliu/Documents/Mendeley Desktop/4390-hogwild-a-lock-free-approach-to-parallelizing-stochastic-gradient-descent.pdf:pdf},
isbn = {9781618395993},
journal = {Advances in Neural Information Processing Systems},
keywords = {incremental gradient methods,machine learning,multicore,parallel computing},
number = {1},
pages = {21},
title = {{HOGWILD!: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent}},
url = {http://arxiv.org/abs/1106.5730},
year = {2011}
}
@article{shalev2013stochastic,
	title={Stochastic dual coordinate ascent methods for regularized loss minimization},
	author={Shalev-Shwartz, Shai and Zhang, Tong},
	journal={Journal of Machine Learning Research},
	volume={14},
	number={Feb},
	pages={567--599},
	year={2013}
}
@inproceedings{johnson2013accelerating,
	title={Accelerating stochastic gradient descent using predictive variance reduction},
	author={Johnson, Rie and Zhang, Tong},
	booktitle={Advances in Neural Information Processing Systems},
	pages={315--323},
	year={2013}
}
@article{boyd2011distributed,
	title={Distributed optimization and statistical learning via the alternating direction method of multipliers},
	author={Boyd, Stephen and Parikh, Neal and Chu, Eric and Peleato, Borja and Eckstein, Jonathan},
	journal={Foundations and Trends{\textregistered} in Machine Learning},
	volume={3},
	number={1},
	pages={1--122},
	year={2011},
	publisher={Now Publishers Inc.}
}
@incollection{bottou2010large,
	title={Large-scale machine learning with stochastic gradient descent},
	author={Bottou, L{\'e}on},
	booktitle={Proceedings of COMPSTAT'2010},
	pages={177--186},
	year={2010},
	publisher={Springer}
}
@inproceedings{defazio2014saga,
	title={Saga: A fast incremental gradient method with support for non-strongly convex composite objectives},
	author={Defazio, Aaron and Bach, Francis and Lacoste-Julien, Simon},
	booktitle={Advances in Neural Information Processing Systems},
	pages={1646--1654},
	year={2014}
}
@article{allen2016katyusha,
	title={Katyusha: Accelerated variance reduction for faster SGD},
	author={Allen-Zhu, Zeyuan},
	journal={ArXiv e-prints, abs/1603.05953},
	year={2016}
}
@article{lan2015optimal,
	title={An optimal randomized incremental gradient method},
	author={Lan, Guanghui and Zhou, Yi},
	journal={arXiv preprint arXiv:1507.02000},
	year={2015}
}
@inproceedings{zhang2015stochastic,
	title={Stochastic Primal-Dual Coordinate Method for Regularized Empirical Risk Minimization.},
	author={Zhang, Yuchen and Lin, Xiao},
	booktitle={ICML},
	pages={353--361},
	year={2015}
}
@inproceedings{lin2014accelerated,
	title={An accelerated proximal coordinate gradient method},
	author={Lin, Qihang and Lu, Zhaosong and Xiao, Lin},
	booktitle={Advances in Neural Information Processing Systems},
	pages={3059--3067},
	year={2014}
}
@inproceedings{shalev2014accelerated,
	title={Accelerated proximal stochastic dual coordinate ascent for regularized loss minimization.},
	author={Shalev-Shwartz, Shai and Zhang, Tong},
	booktitle={ICML},
	pages={64--72},
	year={2014}
}
@article{xiao2014proximal,
	title={A proximal stochastic gradient method with progressive variance reduction},
	author={Xiao, Lin and Zhang, Tong},
	journal={SIAM Journal on Optimization},
	volume={24},
	number={4},
	pages={2057--2075},
	year={2014},
	publisher={SIAM}
}
@inproceedings{shalev2013accelerated,
	title={Accelerated mini-batch stochastic dual coordinate ascent},
	author={Shalev-Shwartz, Shai and Zhang, Tong},
	booktitle={Advances in Neural Information Processing Systems},
	pages={378--385},
	year={2013}
}
@article{ouyang2013stochastic,
	title={Stochastic Alternating Direction Method of Multipliers.},
	author={Ouyang, Hua and He, Niao and Tran, Long and Gray, Alexander G},
	journal={ICML (1)},
	volume={28},
	pages={80--88},
	year={2013}
}
@inproceedings{suzuki2013dual,
	title={Dual Averaging and Proximal Gradient Descent for Online Alternating Direction Multiplier Method.},
	author={Suzuki, Taiji and others},
	booktitle={ICML (1)},
	pages={392--400},
	year={2013}
}
@inproceedings{recht2011hogwild,
	title={Hogwild: A lock-free approach to parallelizing stochastic gradient descent},
	author={Recht, Benjamin and Re, Christopher and Wright, Stephen and Niu, Feng},
	booktitle={Advances in Neural Information Processing Systems},
	pages={693--701},
	year={2011}
}
@inproceedings{zinkevich2010parallelized,
	title={Parallelized stochastic gradient descent},
	author={Zinkevich, Martin and Weimer, Markus and Li, Lihong and Smola, Alex J},
	booktitle={Advances in neural information processing systems},
	pages={2595--2603},
	year={2010}
}
@inproceedings{yang2013trading,
	title={Trading computation for communication: Distributed stochastic dual coordinate ascent},
	author={Yang, Tianbao},
	booktitle={Advances in Neural Information Processing Systems},
	pages={629--637},
	year={2013}
}
@inproceedings{zhang2014asynchronous,
	title={Asynchronous Distributed ADMM for Consensus Optimization.},
	author={Zhang, Ruiliang and Kwok, James T},
	booktitle={ICML},
	pages={1701--1709},
	year={2014}
}
@inproceedings{roux2012stochastic,
	title={A stochastic gradient method with an exponential convergence \_rate for finite training sets},
	author={Roux, Nicolas L and Schmidt, Mark and Bach, Francis R},
	booktitle={Advances in Neural Information Processing Systems},
	pages={2663--2671},
	year={2012}
}
@inproceedings{defazio2016simple,
	title={A simple practical accelerated method for finite sums},
	author={Defazio, Aaron},
	booktitle={Advances In Neural Information Processing Systems},
	pages={676--684},
	year={2016}
}
@article{yang2013analysis,
	title={Analysis of distributed stochastic dual coordinate ascent},
	author={Yang, Tianbao and Zhu, Shenghuo and Jin, Rong and Lin, Yuanqing},
	journal={arXiv preprint arXiv:1312.1031},
	year={2013}
}
@inproceedings{zinkevich2010parallelized,
	title={Parallelized stochastic gradient descent},
	author={Zinkevich, Martin and Weimer, Markus and Li, Lihong and Smola, Alex J},
	booktitle={Advances in neural information processing systems},
	pages={2595--2603},
	year={2010}
}
@inproceedings{recht2011hogwild,
	title={Hogwild: A lock-free approach to parallelizing stochastic gradient descent},
	author={Recht, Benjamin and Re, Christopher and Wright, Stephen and Niu, Feng},
	booktitle={Advances in Neural Information Processing Systems},
	pages={693--701},
	year={2011}
}
@inproceedings{agarwal2011distributed,
	title={Distributed delayed stochastic optimization},
	author={Agarwal, Alekh and Duchi, John C},
	booktitle={Advances in Neural Information Processing Systems},
	pages={873--881},
	year={2011}
}
@article{gabay1976dual,
	title={A dual algorithm for the solution of nonlinear variational problems via finite element approximation},
	author={Gabay, Daniel and Mercier, Bertrand},
	journal={Computers \& Mathematics with Applications},
	volume={2},
	number={1},
	pages={17--40},
	year={1976},
	publisher={Elsevier}
}