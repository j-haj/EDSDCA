Automatically generated by Mendeley Desktop 1.17.9
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@article{Shalev-Shwartz2016a,
abstract = {Stochastic Dual Coordinate Ascent is a popular method for solving regularized loss minimization for the case of convex losses. We describe variants of SDCA that do not require explicit regularization and do not rely on duality. We prove linear convergence rates even if individual loss functions are non-convex, as long as the expected loss is strongly convex.},
archivePrefix = {arXiv},
arxivId = {1602.01582},
author = {Shalev-Shwartz, Shai},
eprint = {1602.01582},
file = {:Users/mingrliu/Documents/github/EDSDCA/proposal-template/references/shalev-shwartza16.pdf:pdf},
isbn = {9781510829008},
journal = {Proceedings of The 33rd International Conference on Machine Learning},
pages = {747--754},
title = {{SDCA without Duality, Regularization, and Individual Convexity}},
url = {http://arxiv.org/abs/1602.01582},
volume = {48},
year = {2016}
}
@article{Shalev-Shwartz2016,
abstract = {Stochastic Gradient Descent (SGD) has become popular for solving large scale supervised machine learning optimization problems such as SVM, due to their strong theoretical guarantees. While the closely related Dual Coordinate Ascent (DCA) method has been implemented in various software packages, it has so far lacked good convergence analysis. This paper presents a new analysis of Stochastic Dual Coordinate Ascent (SDCA) showing that this class of methods enjoy strong theo-retical guarantees that are comparable or better than SGD. This analysis justifies the effectiveness of SDCA for practical applications.},
archivePrefix = {arXiv},
arxivId = {1209.1873},
author = {Shalev-Shwartz, Shai and Zhang, Tong},
doi = {10.1007/s10107-014-0839-0},
eprint = {1209.1873},
file = {:Users/mingrliu/Documents/github/EDSDCA/proposal-template/references/shalev-shwartz13a.pdf:pdf},
isbn = {1532-4435},
issn = {14364646},
journal = {Mathematical Programming},
keywords = {90C06,90C15,90C25},
number = {1-2},
pages = {105--145},
pmid = {2079951},
title = {{Accelerated proximal stochastic dual coordinate ascent for regularized loss minimization}},
volume = {155},
year = {2016}
}
