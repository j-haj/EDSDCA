\begin{thebibliography}{10}

\bibitem{allen2016katyusha}
Z.~Allen-Zhu.
\newblock Katyusha: Accelerated variance reduction for faster sgd.
\newblock {\em ArXiv e-prints, abs/1603.05953}, 2016.

\bibitem{bottou2010large}
L.~Bottou.
\newblock Large-scale machine learning with stochastic gradient descent.
\newblock In {\em Proceedings of COMPSTAT'2010}, pages 177--186. Springer,
  2010.

\bibitem{boyd2011distributed}
S.~Boyd, N.~Parikh, E.~Chu, B.~Peleato, and J.~Eckstein.
\newblock Distributed optimization and statistical learning via the alternating
  direction method of multipliers.
\newblock {\em Foundations and Trends{\textregistered} in Machine Learning},
  3(1):1--122, 2011.

\bibitem{defazio2014saga}
A.~Defazio, F.~Bach, and S.~Lacoste-Julien.
\newblock Saga: A fast incremental gradient method with support for
  non-strongly convex composite objectives.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1646--1654, 2014.

\bibitem{johnson2013accelerating}
R.~Johnson and T.~Zhang.
\newblock Accelerating stochastic gradient descent using predictive variance
  reduction.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  315--323, 2013.

\bibitem{lan2015optimal}
G.~Lan and Y.~Zhou.
\newblock An optimal randomized incremental gradient method.
\newblock {\em arXiv preprint arXiv:1507.02000}, 2015.

\bibitem{lin2014accelerated}
Q.~Lin, Z.~Lu, and L.~Xiao.
\newblock An accelerated proximal coordinate gradient method.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  3059--3067, 2014.

\bibitem{nemirovski2009robust}
A.~Nemirovski, A.~Juditsky, G.~Lan, and A.~Shapiro.
\newblock Robust stochastic approximation approach to stochastic programming.
\newblock {\em SIAM Journal on optimization}, 19(4):1574--1609, 2009.

\bibitem{shalev2013accelerated}
S.~Shalev-Shwartz and T.~Zhang.
\newblock Accelerated mini-batch stochastic dual coordinate ascent.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  378--385, 2013.

\bibitem{shalev2013stochastic}
S.~Shalev-Shwartz and T.~Zhang.
\newblock Stochastic dual coordinate ascent methods for regularized loss
  minimization.
\newblock {\em Journal of Machine Learning Research}, 14(Feb):567--599, 2013.

\bibitem{shalev2014accelerated}
S.~Shalev-Shwartz and T.~Zhang.
\newblock Accelerated proximal stochastic dual coordinate ascent for
  regularized loss minimization.
\newblock In {\em ICML}, pages 64--72, 2014.

\bibitem{xiao2014proximal}
L.~Xiao and T.~Zhang.
\newblock A proximal stochastic gradient method with progressive variance
  reduction.
\newblock {\em SIAM Journal on Optimization}, 24(4):2057--2075, 2014.

\bibitem{zhang2015stochastic}
Y.~Zhang and X.~Lin.
\newblock Stochastic primal-dual coordinate method for regularized empirical
  risk minimization.
\newblock In {\em ICML}, pages 353--361, 2015.

\end{thebibliography}
